---
title:  "Who gatekeeps the gatekeepers"  
tags: [ai, tech, data, gatekeeping, math]
published: false
---


## Takeaways

1.
2.
3. Feature post by Vicki Boykis on how we should pay attention to the man behind the AI curtain

<style>
      .iframe-container {
        overflow: hidden;        
        padding-top: 50%; <!-- Calculated from the aspect ration of the content (in case of 16:9 it is 9/16= 0.5625) -->
        position: relative;
      }
      .iframe-container iframe { 
         border: 0;
         height: 100%; <!-- Finally, width and height are set to 100% so the iframe takes up 100% of the containers space. -->
         left: 0;
         position: absolute;
         top: 0;
         width: 100%;
         display: block;
         margin: 0 auto; <!-- center image -->
      }
      <!-- 4x3 Aspect Ratio -->
      .iframe-container-4x3 {
        padding-top: 75%;
      }
</style> 

<div class="iframe-container-4x3">
  <p align="center"><iframe src="https://avoidboringpeople.substack.com/embed" frameborder="0" scrolling="no"> </iframe></p>
</div>

## 1. Gatekeeping for group vs individual status

You've probably seen this play out before.

A beginner, starry eyed and bushy tailed, will come looking for advice on how to get started in a subject.

![post]({{ site.url }}{{ site.baseurl }}/assets/images/gatekeep/gatekeep 1.png)

And a host of experts will then emerge from the depths to tell them it can't be done, that they should go back and spend years learning the basics, and they should be embarrassed for asking the question in the first place. *"The nerve of some people, thinking they could avoid paying their dues."*

![post]({{ site.url }}{{ site.baseurl }}/assets/images/gatekeep/gatekeep 2.png)

With some even finding things to complain about when others launch courses to help the beginners do just that.

![post]({{ site.url }}{{ site.baseurl }}/assets/images/gatekeep/gatekeep 3.png)

We encounter gatekeeping all the time, and it's mostly done to preserve status. There are some valid forms of gatekeeping, and I'll come to that in a bit. But it's nearly always done to exclude people and be mean. Funnily enough, the gatekeepers never seem to realise that they could be excluded too.

For example, you could say that you can't understand machine learning unless you learn calculus, statistics, and linear algebra, just like the commenter above.

![post]({{ site.url }}{{ site.baseurl }}/assets/images/gatekeep/gatekeep 4.png)

And you could also say that you can't understand linear algebra unless you learn group theory, how [matrices are a ring](https://www.youtube.com/watch?v=_RTHvweHlhE "ring"), and [whether we're working with linear groups or not](https://www.youtube.com/watch?v=AJTRwhSZJWw "group") \[1\]

![post]({{ site.url }}{{ site.baseurl }}/assets/images/gatekeep/gatekeep 5.png)

And you could gatekeep further and say that the above depends on [set theory](https://plato.stanford.edu/entries/set-theory/ "set"), [Peano axioms](https://en.wikipedia.org/wiki/Peano_axioms "Peano"), and [philosophy](https://plato.stanford.edu/entries/philosophy-mathematics/ "philo"). Wonder how much of that the commenter spent his undergrad studying.

If we wanted to, we could gatekeep anything. 

**We'd also never get anywhere since we'd never get started.**  

I do believe there are valid forms of gatekeeping. If you're excluding someone because they would be harmful to the community, that's a reasonable action to take. For example, if you're building a community of people who play video games, and you get a request from someone who wants to ban video games, probably doesn't make sense to accept her in.

More often though, gatekeeping is an attempt by individuals in the "in-group" to maintain their status, as if they somehow lose out the more people know what they know. It comes from a place of insecurity. As I wrote about last month, that's like [shorting the lottery](https://avoidboringpeople.substack.com/p/it-only-goes-up-from-here "short"), and probably a bad idea.

Now, note that the gatekeeper's aren't totally wrong. In fact, their suggestions often make sense in a certain context. For example, it would be tremendously helpful to know linear algebra while studying machine learning. And if you want to become an expert, you have to master all the math required \[2\]. But artificially preventing people from starting a subject doesn't help anyone. A better response would have been "Yes, here's some simpler courses to get started, come back and revisit the fundamentals after." Enable rather than disable.

![post]({{ site.url }}{{ site.baseurl }}/assets/images/gatekeep/gatekeep 5.png)

If you've been biased towards gatekeeping, I'd encourage thinking about whether you're helping the community, or helping yourself. You can do better. Fields are large enough that the more people are pursuing it, the better. It's rarely, if ever, a zero-sum game. By being open-minded and accepting of beginners, we can grow the entire pie, and have more for ourselves. That's how we can drive progress as individuals.

Open more gates, and [be kind.](https://www.youtube.com/watch?v=xnouj9Yz-Gs&feature=youtu.be&t=23 "who")

## 2. 

## 3. Pay attention to the man behind the curtain

This month's guest post is from Vicki Boykis of [Normcore Tech](https://vicki.substack.com/ "vicki"), a newsletter about making tech less sexy, more boring, and anything adjacent to tech that the mainstream media isn't covering. Vicki's excellent for anything data related, and the one memory that sticks out the most for me is when she predicted Netflix would introduce a "top 10 most popular" row. 

Here's Vicki:

![post]({{ site.url }}{{ site.baseurl }}/assets/images/gatekeep/gatekeep 00.jpeg)

If you've ever seen a Boston Dynamics video, it's easy to be convinced that algorithm-wielding robots are going to transform our society into an unrecognizable, metallic, cyber dystopia.

Boston Dynamics, now a subsidiary of SoftBank, became famous for their work on robots like BigDog - elegant machines that can walk and move with terrifying anthropomorphic flexibility. Early videos featured hilarious failures and stilted movements. But, the company's latest robots are eerily approaching the uncanny valley of robotic-human movement by doing flips, navigate deftly around furniture, and open doors with nightmarish neck-claws.

News stories tell us to respect the awesome capabilities artificial intelligence, or the ability for computers to autonomously solve problems on the same level as humans. Our society's relationship as a society with AI has been not unlike Dorothy's with the Wizard of Oz. We've been told we need to make the long, dangerous, exhausting trip to find a powerful, omnipotent being who can grant all of our wishes.

The public response to Boston Dynamics robots is the clearest evidence of that. Blaring headlines call their progress "astounding and terrifying." 'These headlines seem scary because there's an element of powerful, smart, unknowable machines deciding the fate of humans, for the worse. But if we peel back the layers, we find that the real problem is not machines. The problem is people.

Take, for example, a 2018 case where, during an Uber autonomous vehicle pilot program, a self-driving car struck and killed a pedestrian during tests in Arizona. A Volvo SUV was "driving" in Tempe, with an operator in the driver's seat. She was not driving, but was monitoring the vehicle. It was almost ten p.m. at night. The pedestrian was crossing the road, unanticipated, with a bicycle, in a place where there was no crosswalk, and the car detected the pedestrian only 5.6 seconds before impact.

The initial media and public response was one of shock, disbelief, and fear that robots were taking over the roads, and that this was a bleak path forward for humanity. However, after months of local and federal investigation, a different and very complicated picture emerged. Recently, the National Transportation Safety Board conducted an investigation and [released](https://www.ntsb.gov/news/events/Documents/2019-HWY18MH010-BMG-abstract.pdf) the detailed case incident, and the report paints a complex picture of many interlaced factors going wrong at the same time.

There were several issues at play. First, in the seconds right before the collision, the operator was looking at her cell phone, and not at the road. Second, with the way the car was built, there was no way for the AI system to activate the breaks. Only the human could do so, but because she was distracted, she didn't. Finally, the system itself didn't recognize that there was a pedestrian on the road until five seconds before it was too late, because it wasn't built to recognize jaywalkers.

There are a lot of illuminating lessons here. First, the algorithm developed for steering the car was missing some key information. In order for a car to be able to recognize specific objects, it has to be fed lots of other pictures of those same objects to infer from. In machine learning this is known as a training set. How were the engineers and data scientists who created the training set to know that they should have included lots of pictures of people jaywalking, at night? Moreover, even if they had included them, what other scenarios would they have to anticipate, in order to cover the whole range of things that can happen when a car is driving? Presumably, here, they should have been working with safety experts and law enforcement officials before the car was ever on the road.

But often, in corporations where there are tight loops between R&D and putting products out into the market, such as there is in the competitive self-driving vehicle industry, machine learning and research teams are constantly under pressure to perform and put out new, impressive results, with little time to reflect, and with little input from anyone other than immediate managers and executives.

Ultimately, if leadership says cars have to go out as soon as possible, there's no time for engineers to think through all of the ramifications of algorithms, and certainly no time, or incentive, to include outside stakeholders.

Which is why, if the algorithm was imperfect, the manual operator and other safety systems should have helped. But, as the report noted, Uber Advanced Technologies Group, at the time, had "an inadequate safety culture," That is, the group had no personnel with backgrounds in safety management and no risk assessment mechanisms within the physical car itself. And, because there was no culture of safety, there was no way for the algorithm to engage the breaks.

So, if the algorithm failed and the autonomous breaking system failed, the operator should have been the safety valve and been easily able to stop what was happening. But, she wasn't paying attention.

And so it wasn't the algorithm itself that was the core issue, but a number of very human pressures: Uber's corporate safety culture at the time, go-to-market pressures, the lack of domain experts involved in reviewing the algorithm, the lack of safety valves in the car, the lack of operator preparedness, and the unpredictability of a pedestrian crossing a multi-lane highway late at night.

And if Uber couldn't (or wouldn't) get this right, then how can other companies operating under the same market pressures and with the same people issues be expected to? Having worked on a range of machine learning projects, I can say with confidence that technical issues usually pale in comparison to problems of corporate politics, and of multiple, compounded problems that no one saw coming in the system. AI systems in and of themselves are very complicated technologies, with many moving parts and many places where they can fail. But when you add people on top, it becomes almost impossible to police.

People rush work due to internal deadlines. People misunderstand metrics. And perhaps most importantly, executives look mainly at immediate financial incentives and deadlines as opposed to examining the holistic picture that may result in short-term losses, but long-term gains.

Show me the company's financial incentives, and I'll show you how and why it acts the way it does. In another example, recently, Netflix changed its recommendation algorithm, which picks what to show you on the home screen, so that the "viewing " metric meant watching at least two minutes of a [title](https://www.theverge.com/2020/1/21/21075927/netflix-the-witcher-viewer-numbers-length-metrics-q4-2019-earnings), from "watched 70 percent of a single episode of a series" to anyone who "chose to watch and did watch for at least 2 minutes."

What this probably means in reality is that the market for streamable content is becoming extremely saturated. Previously, Netflix was in the lead, but now it has to compete with a number of services, including, Disney+. It's under pressure to perform well with its original content, which it spent a lot of time and money on, which means changing the numbers to try to better reflect that.

It's usually these incentives and pressures, more so than the actual components of AI algorithms, that we have to think about when we read and evaluate stories of AI dystopian woe in the news. While the algorithms are, for sure, hard to get right, what's even harder is to predict and dig into the behaviors of these algorithms in conjunction with the systems of people who shepherd them out into the larger world. And that, not the robots, is the concerning part.

When Toto pulls back the curtain on the wizard, he's angered, and shouts, "Pay no attention to the man behind the curtain," as he frantically tries to turn the knobs on the great machine he's built.

In reality, it turns out that the Wizard of Oz is not unlike most AI systems: a great, terrifying machine, propped up entirely by people with their own motivations. But, just like companies need to improve their accountability, so too, we the public, need to start being more like Toto, more critically reading news stories and pulling back the curtain on what's truly going on behind the scenes instead of working off knee-jerk reactions. And, the more media offers us these second-level insights on the people systems behind the AI systems, the better equipped for the robotic future we'll all be.

### Questions for Vicki

**1. What are you practicing deliberately? E.g in the same way a football player would train drills to consciously improve**

Trying not to read Twitter before bed. I was able to do this for 2 weeks before I reinstalled the app on my phone. I’m going to try again next month. I hate that I am dependent on my dopamine reactions to the news and need to get better about it. 

And another one is knee-jerk reacting to news events as they happen. I’ve gotten better about not quote-tweeting stuff or posting about it as it happens and letting things unspool for a week or so, so I can digest and cover in my newsletter. 

And a third is taking a walk every day. 

**2. What are the top frameworks you use to understand the world?**

1. [Good things don't scale. ](http://blog.vickiboykis.com/2017/05/10/good-things-don't-scale/)
2. [Algorithms are driven by the business not by the science](https://vicki.substack.com/p/the-reign-of-big-recsys)
3. [Fix the internet by writing good stuff and being nice to people](http://blog.vickiboykis.com/2016/11/20/fix-the-internet/)
4. [The news is terrible at explaining the news](https://vicki.substack.com/p/why-dont-we-get-the-news-we-need)
5. [A lot of business culture is more about sales than delivery](https://vicki.substack.com/p/between-sales-and-execution-culture)
6. [A group of people on the internet will try to tear each other apart](https://vicki.substack.com/p/no-one-talks-about-teams)

**3. What do you wish more people ask you about?**

What it's like to be a working parent, particularly mom, during COVID. [There is an enormous swath of women](https://www.bloomberg.com/news/articles/2020-10-02/women-drop-out-of-workforce-at-fastest-pace-since-pandemic-peak) who are dropping out of the workforce to now handle their kids' education and it's not an issue we're paying enough attention to

## Shoutouts

1. Over 16,000 innovative startup and tech professionals subscribe to Charlie O’Donnell’s weekly newsletter, which started over ten years ago, before newsletters were cool. He shares info on fundraising, startup strategy, and maintains a huge list of high-quality weekly events for anyone working on either the next big thing or just their next big thing. Signup here: [www.nycweeklynewsletter.com](www.nycweeklynewsletter.com "link")
2. I spoke with the founder of [Spendyt](https://spendyt.com/ "spendyt"), a simple way to track your budget in an email-optional way
3. I caught up with the founder of [Explo](https://explo.co/ "explo"), a YC company that enables companies to build user-facing analytics, like dashboards and reports, seamlessly integrated into your web app
4. I chatted with the CEO of [Savitude](https://www.savitude.com/ "Sav"), which is using AI to design clothes faster

## Other

1. [A brief history of graphics (youtube video)](https://www.youtube.com/watch?v=QyjyWUrHsFc&list=WL&index=8 "gfx")
2. ["Colour blindness is an inaccurate term"](https://commandcenter.blogspot.com/2020/09/color-blindness-is-inaccurate-term.html "colour"). As a colour blind person this was cool to read, especially since it had new info
3. ["The long tail turns out toe be a major cause of the economic challenges of building AI businesses"](https://a16z.com/2020/08/12/taming-the-tail-adventures-in-improving-ai-economics/ "a16z")
4. [Intro to abstract algebra and group theory by Socratica (youtube series)](https://www.youtube.com/watch?v=IP7nW_hKB7I "aa"). Recommneded, suitable for beginners.
5. ["Fusion reactor very likely to work"](https://futurism.com/mit-researchers-fusion-reactor-very-likely-work "fusion")

## Survey

It's not too late to take the 2 min [reader survey](https://leonlinsxtypeform.typeform.com/to/jMJ7PG3e "survey")! So far I've gotten responses describing the newsletter as "The most bizarre interesting discussion on popular topics" (a glowing review in my books) to "Not sure, honestly not differentiated" (not so great), reminding me of the tweet below. More responses are much appreciated! 

![post]({{ site.url }}{{ site.baseurl }}/assets/images/gatekeep/gatekeep 0.png)

## Footnotes

1. Having not known about group theory until this year, I'd have to say it's the most fascinating thing I've learnt about the entire year. It really helped in giving me intuition behind why we define alegra "things" and "operations" the way we do. Why is matrix multiplication not commutative, for example? 
2. To be clear - I agree that in order to become good at machine learning, you'll want to be good at calculus, stats, linear algebra etc. The commenter is right in that regard. I disagree that we should gatekeep people for years because they haven't learnt all that's needed

*If you liked this, you'll like [my monthly newsletter.](https://avoidboringpeople.substack.com/ "ABP") Sign up here:*

<div class="iframe-container-4x3">
  <p align="center"><iframe src="https://avoidboringpeople.substack.com/embed" frameborder="0" scrolling="no"> </iframe></p>
</div>
